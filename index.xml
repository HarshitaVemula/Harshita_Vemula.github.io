<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Akhilesh Reddy on Akhilesh Reddy</title>
    <link>https://akhilesh-reddy.github.io/</link>
    <description>Recent content in Akhilesh Reddy on Akhilesh Reddy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 -0500</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Recommender system using Bayesian personalized ranking</title>
      <link>https://akhilesh-reddy.github.io/post/bpr/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/bpr/</guid>
      <description>&lt;p&gt;This article is originally published in Towards Data science. Click &lt;a href=&#34;https://towardsdatascience.com/recommender-system-using-bayesian-personalized-ranking-d30e98bba0b9&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to read the article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Twitter does it? Challenges in implementing recommender systems at scale</title>
      <link>https://akhilesh-reddy.github.io/post/twitter/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/twitter/</guid>
      <description>&lt;p&gt;This article is originally published in Towards Data science. Click &lt;a href=&#34;https://towardsdatascience.com/how-twitter-does-it-challenges-in-implementing-recommender-systems-at-scale-353f7a1ae4ab&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to read the article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sketch recognition using Mobilenet</title>
      <link>https://akhilesh-reddy.github.io/post/sketch-recognition/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/sketch-recognition/</guid>
      <description>&lt;p&gt;This article is originally published in Towards Data science. Click &lt;a href=&#34;https://towardsdatascience.com/doodling-with-deep-learning-1b0e11b858aa&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to read the article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scraping Reddit data using Python and Google BigQuery</title>
      <link>https://akhilesh-reddy.github.io/post/medium/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/medium/</guid>
      <description>&lt;p&gt;This article is originally published in Towards Data science. Click &lt;a href=&#34;https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to read the article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ML Algorithms 4:Introduction to Boosting and the famous XGBoost</title>
      <link>https://akhilesh-reddy.github.io/post/xgboost-and-lightxgboost/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/xgboost-and-lightxgboost/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;I am writing these series of posts to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others in an intuitive way&lt;/strong&gt;&lt;br /&gt;
Boosting models are some of the most famous machine learning algorithms. There fame comes from the fact that they learn from their mistakes during the process sequentially.
This helps those models to actually build on top of the mistakes and give faster and better results.&lt;/p&gt;

&lt;p&gt;In this post, we will be looking into the following aspects:&lt;br /&gt;
1. Introduction to different kinds of boosting models and how they work&lt;br /&gt;
2. What are the reasons for the fame of XGBoost&lt;/p&gt;

&lt;h3 id=&#34;boosting&#34;&gt;Boosting:&lt;/h3&gt;

&lt;p&gt;We all know boosting models belong to the family of ensemble methods which tend to produce strong learners by combining multiple weak learners.
Following is the picture that briefly explains the different kinds of ensembling.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/ensembling.png&#34; alt=&#34;ensembling&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Boosting is mainly different from bagging for one and only reason - sequential learning. In boosting, the model learns from its mistakes sequentially whereas in bagging the ensembling happens at the last step.
Here, we take the residuals of the first model and build model on top of the residuals sequentially to identify the patterns that are not identified by the first model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/gradient boosting.png&#34; alt=&#34;Gradient boosting&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;adaboost&#34;&gt;Adaboost:&lt;/h3&gt;

&lt;p&gt;Adaboost,short for Adaptive boosting, is a boosting model that is sensitive to the errors in the previous models. At each step, the model would be penalized for instances of misclassification and would be forced to make the right decision in the next iteration to reduce the total error.In Adaboost, this is traditionally performed by adding weights to the errors that occur at each step of the model.
You can see below at each step, the decision stump is decided based on the weights assigned to each point. This changes continuously to reduce the final error.
In the final step, all these decisions combined together will give the classification of the dataset.&lt;br /&gt;
For detailed mathematical explanation, you can refer &lt;a href=&#34;https://en.wikipedia.org/wiki/AdaBoost&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/adaboost.png&#34; alt=&#34;Adaboost&#34; /&gt;&lt;/p&gt;

&lt;p&gt;However, there are some limitations to Adaboost model:&lt;br /&gt;
1. They are sensitive to noisy and pose the risk of assigning more weight to noise&lt;br /&gt;
2. The same problem would be applicable to outliers also&lt;/p&gt;

&lt;p&gt;This takes us to our next model.&lt;/p&gt;

&lt;h3 id=&#34;gradient-boosting&#34;&gt;Gradient Boosting:&lt;/h3&gt;

&lt;p&gt;In Adaboost, the primary task is to minimize the loss and one of the best ways to do that is to find the global minima of the loss function using a gradient descent model.
Gradient boosting trees use gradient descent optimization method to optimize the parameters by fitting a new model at each iteration.
This class of algorithms were also described as a stage-wise additive model. This is because one new weak learner is added at a time and existing weak learners in the model are frozen and left unchanged.&lt;/p&gt;

&lt;p&gt;Gradient boosting is an additive model. In a general gradient descent application like regression, we optimize the parameters to achieve minimized loss. But in this case, we add one tree at each step and optmize the parameters of the tree that give minimum residual loss at that step.
This is generally called gradient descent in functional space. This process is repeated at each step till we arrive at the specified number of trees to be used or by early stopping.&lt;/p&gt;

&lt;p&gt;By the end of the final iteration, we would have arrived at a model that had learnt from the mistakes of the previous models sequentially and gives out the best predictions.&lt;br /&gt;
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/iter1.png&#34; alt=&#34;iter1&#34; /&gt;
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/iter50.png&#34; alt=&#34;iter50&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We could should be cautious of overfitting as we increase the number of iterations for a given dataset and this warrants the use of regularization parameters.&lt;/p&gt;

&lt;p&gt;Key features:&lt;br /&gt;
1. Any differentiable loss function can be optimized using gradient descent method&lt;br /&gt;
2. Gradient descent happens in functional space&lt;br /&gt;
3. Parameters of the tree are often set through cross validation&lt;br /&gt;
4. Early stopping is required to avoid overfitting&lt;/p&gt;

&lt;h3 id=&#34;xgboost&#34;&gt;XGBoost:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/xgb1.png&#34; alt=&#34;JCM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you are part of the Kaggle eco system, then there is high chance that you would have heard about XGboost algorithm and its potential to deliver results with high speed and performance.&lt;/p&gt;

&lt;p&gt;XGboost,short for Extreme gradient boosting, is an algorithm that is designed for increasing speed and performance using Gradient boosting trees. &lt;a href=&#34;https://homes.cs.washington.edu/~tqchen/&#34; target=&#34;_blank&#34;&gt;Tianqi chen&lt;/a&gt; developed this algorithm while he is doing his Ph.d in University of Washington.
XGboost is an implementation of gradient boosting trees which increases the speed and performance of the model by taking advantage of parallelization and distributed computing.&lt;/p&gt;

&lt;p&gt;Most important features of XGBoost can be divided into&lt;br /&gt;
&lt;strong&gt;System features&lt;/strong&gt;&lt;br /&gt;
It enables of the parallelization of tree construction using all cores during training&lt;br /&gt;
Distributed computing is made possible using cluster of machines using XGBoost
It employs the concept of cache optimization which makes it more efficient&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithmic features&lt;/strong&gt;&lt;br /&gt;
1.Sparse aware: It automatic handling of missign data values&lt;br /&gt;
2.Block structure: It supports parallelization of tree structures as mentioned already in the system features&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key points to remember while running an XGBoost model:&lt;/strong&gt;&lt;br /&gt;
1. Avoid overfitting by using early stopping&lt;br /&gt;
2. Use cross validation while pruning the trees&lt;br /&gt;
3. Use xgb.Dmatrix to create train and test datasets&lt;br /&gt;
4. Always perform hyperparameter search(Random search would work well than Grid search)&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all folks. See you later in the next article.
Happy learning!&lt;/p&gt;

&lt;p&gt;References:&lt;br /&gt;
[1] &lt;a href=&#34;https://towardsdatascience.com/boosting-in-machine-learning-and-the-implementation-of-xgboost-in-python-fb5365e9f2a0&#34; target=&#34;_blank&#34;&gt;https://towardsdatascience.com/boosting-in-machine-learning-and-the-implementation-of-xgboost-in-python-fb5365e9f2a0&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&#34;https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/&#34; target=&#34;_blank&#34;&gt;https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/&lt;/a&gt;&lt;br /&gt;
[3] &lt;a href=&#34;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&#34; target=&#34;_blank&#34;&gt;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&lt;/a&gt;&lt;br /&gt;
[4] &lt;a href=&#34;https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052&#34; target=&#34;_blank&#34;&gt;https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052&lt;/a&gt;&lt;br /&gt;
[5] &lt;a href=&#34;http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/&#34; target=&#34;_blank&#34;&gt;http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/&lt;/a&gt;&lt;br /&gt;
[6] &lt;a href=&#34;http://statweb.stanford.edu/~jhf/ftp/trebst.pdf&#34; target=&#34;_blank&#34;&gt;http://statweb.stanford.edu/~jhf/ftp/trebst.pdf&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;[7] &lt;a href=&#34;https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/&#34; target=&#34;_blank&#34;&gt;https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ML Algorithms 3: Bagging and Random forests </title>
      <link>https://akhilesh-reddy.github.io/post/bagging-and-random-forests/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/bagging-and-random-forests/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;I am writing these series of posts to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others in an intuitive way&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/ensembling.png&#34; alt=&#34;ensembling&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;bagging&#34;&gt;&lt;strong&gt;Bagging&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Bagging is an ensemble technique. In this method, instead of training just one tree, we train hundreds of trees and create an ensemble of the output at the end.
For each tree, instead of taking the entire data, we take only a few datapoints as a bagging sample.&lt;br /&gt;
Each tree is let to grow like any other normal tree. All the methods that are discussed in the &amp;ldquo;Decision trees&amp;rdquo; post are still applicable here.
At the end, all the hundreds of trees give different outputs and all these outputs are generally averaged to come up with the final class or prediction for a particular data point.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It is important to note that trees are not pruned in bagging methods. This is because if each tree overfits and has high variance, bagging will capture information from all these overfitted trees and results in a better output&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;why-bagging&#34;&gt;Why bagging?&lt;/h3&gt;

&lt;p&gt;The main benefits of using this method is that it reduces the chances of overfitting and takes advantage of the weakness of different trees and create a single strong learner.
If we have multiple trees that are uncorrelated, in other words, if they have high variance then the final ensembled output would have a higher accuracy as it is learning different values from different trees and will rightly learn different patterns present in the data.&lt;/p&gt;

&lt;h2 id=&#34;random-forest&#34;&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/rf1.jpg&#34; alt=&#34;rf&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;introduction-1&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Random forest is one of the most famous bagging techniques in the machine learning community.
The main difference between a normal bagging method and Random forest is that instead of considering all variables for a particular tree for the recursive split, we consider only a subset of variables.
At each split, a random sample of m predictors is considered and is generally $\sqrt{P}$ where P is the total number of predictors. Split happens on of these m predictors.
We can decide the size of subset but the variables for each tree are selected in a random manner.&lt;/p&gt;

&lt;p&gt;All the trees present in a random forest can be tuned for hyper parameters that we discussed earlier.
Even the numebr of variables that have to be selected for each tree can be fed into the hyper parameter method.&lt;/p&gt;

&lt;h3 id=&#34;key-points-to-note&#34;&gt;Key points to note:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;It creates a strong learner by combining the outputs from all the weak learners&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Random forest tends to run for longer times when the number of trees are more&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Size of tree and number of trees can be decided using k fold cross validation techniques&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;If there is strong predictor in one tree, most of the trees will use this as a first split resulting in similar trees and correlated outputs. Average of correlated outputs will not lead to much reduction in variance. So it is preferable have weak predictors and use random forests to predict the output.&lt;/li&gt;
&lt;li&gt;Random forest will not overfit even with the increase in sample size (B)&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Variable importance measures plto gives the importance of all the variables after generating the ensemble output from all the trees.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;For classification tasks, majority vote is considered while making the final class selection&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All trees present in a random forest can be tuned for hyper parameters that we discussed earlier.&lt;/p&gt;

&lt;h3 id=&#34;implementation-in-python&#34;&gt;Implementation in Python:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;def random_forest(X,y):

    # Creating the train and test split
    from sklearn.model_selection import train_test_split
    X_train_m, X_val_m, y_train_m, y_val_m = train_test_split(X, y, test_size=0.3, random_state=1)
    
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GridSearchCV
    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import accuracy_score
    model = RandomForestClassifier(n_estimators = 250,max_features = None).fit(X_train_m, y_train_m)
#     grid_params = {&#39;criterion&#39;: [&#39;gini&#39;], &#39;max_features&#39; : [None], &#39;n_estimators&#39;: [300]}
#     para_search = GridSearchCV(model, grid_params, scoring = &#39;accuracy&#39;, cv = 5).fit(X_train_m, y_train_m)
#     best_model = para_search.best_estimator_
    labels_r = model.predict(X_val_m)
    
    train_mse = metrics.mean_squared_error(model.predict(X_train_m), y_train_m)
    test_mse = metrics.mean_squared_error(labels_r, y_val_m)
    print(train_mse)
    print(test_mse)
    print (&#39;Accuracy using Random Forest:&#39;,accuracy_score(y_val_m, labels_r))
    mat_r = confusion_matrix(y_val_m, labels_r)
    print(mat_r)
    return model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the entire code, refer to this &lt;a href=&#34;https://github.com/akhilesh-reddy/Salary-classification-based-on-job-description/blob/master/Salary%20prediction%20based%20on%20job%20description.ipynb&#34; target=&#34;_blank&#34;&gt;notebook&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;implementation-in-r&#34;&gt;Implementation in R:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# Random Forest
rf_model &amp;lt;- randomForest(class_labels_train ~ .,
                         data = X_train,
                         ntree = 1000)
author_predict &amp;lt;- predict(rf_model, X_test_pc, type = &amp;quot;response&amp;quot;)
answer &amp;lt;- as.data.frame(table(author_predict, class_labels_test))
answer$correct &amp;lt;- ifelse(answer$author_predict==answer$class_labels_test, 1, 0)

answer_rf = answer %&amp;gt;% group_by(correct) %&amp;gt;% summarise(&amp;quot;Correct&amp;quot; = sum(Freq))

rf_accuracy &amp;lt;- sum(answer$Freq[answer$correct==1])*100/sum(answer$Freq)
  
print(paste0(&amp;quot;Accuracy is &amp;quot;, rf_accuracy))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the entire code, refer to this &lt;a href=&#34;https://github.com/akhilesh-reddy/Data-Science-Mini-projects/blob/master/Author%20attribution/Author_attribution.md&#34; target=&#34;_blank&#34;&gt;notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all folks! See you in the next article.&lt;/p&gt;

&lt;p&gt;Happy learning!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Music recommendation engine using ALS based Matrix factorization</title>
      <link>https://akhilesh-reddy.github.io/project/music-recommendation/</link>
      <pubDate>Sat, 19 Jan 2019 06:31:29 -0500</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/project/music-recommendation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sketch recognition using Mobilenet</title>
      <link>https://akhilesh-reddy.github.io/project/sketch-recognition/</link>
      <pubDate>Sat, 19 Jan 2019 06:31:29 -0500</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/project/sketch-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visualizing different factors that affect Austin bike sharing and recommendations for bike rebalancing </title>
      <link>https://akhilesh-reddy.github.io/project/austin-bike-sharing/</link>
      <pubDate>Sat, 19 Jan 2019 06:31:29 -0500</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/project/austin-bike-sharing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML Algorithms 2: Decision trees</title>
      <link>https://akhilesh-reddy.github.io/post/decision-trees/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/decision-trees/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;I am writing these series of posts to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others in an intuitive way&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/decision tree.png&#34; alt=&#34;decision tree&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Decision trees are a class of machine learning algorithms that decide the output class of a datapoint based on a series of binary decisions using the variables in the dataset.
This split is generally called binary recrusive split and happens at each step of the tree. A single parent node will be split into 2 child nodes based on a particular variable.&lt;br /&gt;
At the end of it, we obtain the final child nodes which are called leaves. Now the number of leaves that can be created depends on how we long we want the tree to grow.&lt;/p&gt;

&lt;p&gt;For example, if there are a 100 students and each of them have their resident status listed as either resident or non-resident. Now at the first split of the decision tree, the entire 100 students will be split into 2 groups, resident or non-resident based on the data.&lt;/p&gt;

&lt;p&gt;How does this happen?&lt;br /&gt;
Well, in real life scenario we will have more than one variable that describes a particular datapoint like age, sex, occupation e.t.c.&lt;br /&gt;
In this case, there are multiple methods that are used to determine the best variable to be used to split the data at a particular step.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Gini index : Gini index gives a measure of total variance across all the classes and the smaller Gini index indicates higher node purity.&lt;br /&gt;
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/gini index.jpg&#34; alt=&#34;gini&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Entropy : This metric is used to calculate the purity of child nodes that are created after splitting the parent node. Higher the purity of the child nodes, lesser is the entropy.&lt;br /&gt;
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/entropy.png&#34; alt=&#34;entropy&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chi-square : Chi-square value is calculated for each variable for the available data at that step of the decision tree. Higher the chi-square value, higher is the important of the variable.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;hyperparameter-tuning&#34;&gt;Hyperparameter tuning:&lt;/h3&gt;

&lt;p&gt;If the tree grows too large, there is a chance for overfitting. It means that the tree will have high variance and will give high error rates for any new test data. To avoid, we generally perform cross validation on a particular metric like rmse to select the number of leaves that are reasonable to avoid overfitting.
We can have various metrics to find the optimal parameters such as accuracy and misclassification error rate. We can tune multiple other parameters of a tree like maximum number of datapoints in each child node, minimum datapoints required to split a parent node into children nodes e.t.c&lt;/p&gt;

&lt;p&gt;Generally, there are 3 types of search techniques that we use for hyper parameter tuning.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Grid search :&lt;/strong&gt; We specify a list of parameters that we think would be reasonable for the data we have and the algorithm searches for all the combinations.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random search :&lt;/strong&gt; The algorithm picks the combinations randomly and decides on the final combinations that will reduce the cost&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian parameter optimization :&lt;/strong&gt; In this method, the knowledge from the previous combination will be carried to the search of the next combination. In other words, the second combination will be conditional on the output of the first combination and continues accordingly.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All these algorithms run on different folds of cross validation data to decide on the best parameters for the trees.&lt;/p&gt;

&lt;h3 id=&#34;pruning&#34;&gt;Pruning:&lt;/h3&gt;

&lt;p&gt;There is one more method to avoid overfitting of the tree called Pruning. In this method, we let the tree grow to its maximum possible size and at the end we prune the tree based on k-fold cross validation technique.
Pruning method randomly removes branches from the trees and calculates the error rate or misclassification rate of the tree. For every subsequent step,if the error rate increases on removing a particular branch, the branch will be replaced back to the tree.&lt;br /&gt;
If the cross validation error rate decreases, the branch will be permanently removed from the tree and the nodes would become the final nodes at that part of the tree.&lt;br /&gt;
This method is more technically called &amp;ldquo;Cost-complexity pruning&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all folks. See you later in the next article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cable-cord cutter sentiment analysis using Reddit data</title>
      <link>https://akhilesh-reddy.github.io/project/cable-cord-cutters-sentiment-analysis-using-reddit-data/</link>
      <pubDate>Fri, 18 Jan 2019 06:31:29 -0500</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/project/cable-cord-cutters-sentiment-analysis-using-reddit-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Salary prediction based on job description using XGboost</title>
      <link>https://akhilesh-reddy.github.io/project/salary-prediction-based-on-job-description/</link>
      <pubDate>Fri, 18 Jan 2019 06:31:29 -0500</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/project/salary-prediction-based-on-job-description/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Statistics 1: Basics and implementation in python</title>
      <link>https://akhilesh-reddy.github.io/post/statistics/</link>
      <pubDate>Fri, 18 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/statistics/</guid>
      <description>

&lt;p&gt;Often, we get overwhelmed by the breadth of the data science and do not give much emphasis on the basics as much as we shoould. In this post, i will go through the fundamental concepts of statistical analysis.
We will have the implementation of these concepts in python so that we also have a hands-on experience while revisiting the concepts.
Before getting into the concepts, lets look into some housekeeping things.&lt;br /&gt;
1. I will be using python 3.6.5 for this exercise&lt;br /&gt;
2. IDE would be Jupyter notebook&lt;br /&gt;
3. You will need to install scipy.stats package&lt;br /&gt;
    Use pip install scipy if it is not already installed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#To install directly from Jupyter notebook
!pip install scipy
!pip install statistics
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;measures-of-central-tendency&#34;&gt;Measures of Central tendency:&lt;/h2&gt;

&lt;h3 id=&#34;mean&#34;&gt;Mean:&lt;/h3&gt;

&lt;p&gt;Given a distribution of numbers describing a particular event, the average of all those numbers would be the mean of that distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/mean.png&#34; width=&#34;120&#34;/&gt;&lt;/p&gt;

&lt;p&gt;You will notice that there are multiple libraries for performing statistics operations. I will illustrate most of them so that you know what all are out there.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# x can be a series or a column from a dataframe but the syntax will remain same. I will be taking the values in a list to keep it simple for illustration
x = [1,2,3,4,5]


# Using numpy
print(&#39;Mean using numpy:&#39;,np.mean(x))

# Using scipy
print(&#39;Mean using scipy:&#39;,scipy.mean(x))

# Using statistics module in python
print(&#39;Mean using statistics module:&#39;,statistics.mean(x))  

Output:  
Mean using numpy: 3.0
Mean using scipy: 3.0
Mean using statistics module: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;median-and-mode&#34;&gt;Median and Mode:&lt;/h3&gt;

&lt;p&gt;Median is the 50th percentile value in a distribution whereas mode is the value with highest frequency within a distribution.
Both have syntax similar to that of mean. I will be using an illustration from one module to show the code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = [1,2,3,4,5,5,5]

# Using scipy
print(&#39;Median using scipy:&#39;,scipy.median(x))

# Using numpy
print(&#39;Mode using scipy:&#39;,scipy.stats.mode(x))

Output:
Median using scipy: 4.0
Mode using scipy: ModeResult(mode=array([5]), count=array([3]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;standard-deviation&#34;&gt;Standard deviation:&lt;/h3&gt;

&lt;p&gt;Standard deviation represents the spread of a data within a distribution. That is how much a datapoint is deviating from its mean. Generally for a normal distribution, there will be a ~68% confidence that a datapoint lies with +/- 1 standard deviation.
In the equation below, you can see that it is the root of fraction of sum of squares of the deviation of a data point from its mean to total degrees of freedom.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/stddev.png&#34; width=&#34;200&#34;/&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = [1,2,3,4,5]

# Using numpy
print(&#39;Stddev using numpy:&#39;,np.std(x))

# Using scipy
print(&#39;Stddev using scipy:&#39;,scipy.std(x))

# Using statistics module in python : Population standard deviation
print(&#39;Stddev using statistics module:&#39;,statistics.pstdev(x))

# Using statistics module in python: sample standard deviation
print(&#39;Stddev using statistics module:&#39;,statistics.stdev(x))

Output:  
Stddev using numpy: 1.4142135623730951
Stddev using scipy: 1.4142135623730951
Stddev using statistics module: 1.4142135623730951
Stddev using statistics module: 1.5811388300841898
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Statistics module actually gives 2 different types of stddev function population standard deviation and sample standard deviation and have to be used according to the type of data that we have.
However, for numpy and scipy,the default function is population standard deviation&lt;/p&gt;

&lt;h3 id=&#34;variance&#34;&gt;Variance:&lt;/h3&gt;

&lt;p&gt;Variance is the expectation of the standard deviation and is also used to calculate the spread of the data
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/variance.png&#34; width=&#34;200&#34;/&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = [1,2,3,4,5]

# Using numpy
print(&#39;Variance using numpy:&#39;,np.var(x))

# Using scipy
print(&#39;Variance using scipy:&#39;,scipy.var(x))

# Using statistics module in python : Population standard deviation
print(&#39;Variance using statistics module:&#39;,statistics.pvariance(x))

# Using statistics module in python: sample standard deviation
print(&#39;Variance using statistics module:&#39;,statistics.variance(x))

Output:  
Variance using numpy: 2.0
Variance using scipy: 2.0
Variance using statistics module: 2
Variance using statistics module: 2.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fun fact:
Mean and variance satisfy the property of sufficiency which means that they can be used to explain the properties of data. They in a way helps us to reduce the complexity of the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mathematics of sufficiency:&lt;/strong&gt;&lt;br /&gt;
Conditional distribution:&lt;/p&gt;

&lt;h3 id=&#34;key-concepts-before-hypothesis-testing&#34;&gt;Key concepts before hypothesis testing:&lt;/h3&gt;

&lt;p&gt;Before going into the hypothesis tests, we need to be very clear with the terminology and the key basic concepts that will be used in hypothesis testing like significance interval, confidence interval e.t.c.
 In the next section, we will discuss these concepts in detail and then move to the hypothesis tests&lt;/p&gt;

&lt;h4 id=&#34;null-hypothesis&#34;&gt;Null hypothesis:&lt;/h4&gt;

&lt;p&gt;It is a default statement that there is no relationship between two entites or events.&lt;/p&gt;

&lt;h4 id=&#34;alternative-hypothesis&#34;&gt;Alternative hypothesis:&lt;/h4&gt;

&lt;p&gt;Hypothesis that we support when we reject the null hypothesis based on the significance of the results.&lt;/p&gt;

&lt;h4 id=&#34;test-statistic&#34;&gt;test-statistic:&lt;/h4&gt;

&lt;p&gt;A test-statistic is a random variable that is calculated from the data to be used in the hypothesis testing. It is used to calculate the p-value based on which we reject or accept the null hypothesis.&lt;/p&gt;

&lt;h4 id=&#34;p-value&#34;&gt;p_value:&lt;/h4&gt;

&lt;p&gt;P value talks abuot the statistical significance of the results of a hypothesis test. Generally, 0.05 is a standard p-value that statisticians consider while performing the hypothesis test. If the p-value is less than 0.05, we reject the null hypothesis, but if the p-value is
higher than 0.05, we concude that we do not have evidence to reject the null hypothesis. It also corresponds to 95% confidence interval. In some clinial trials, statisticians even consider 0.01 p-value due to the need of higher confidence interval.&lt;/p&gt;

&lt;p&gt;For example, in terms of linear regression, if want to test if there is a relationship between independent variable and dependent variable we can do hypothesis testing.Our null hypothesis would be that there is no relation between the variables and
the alternative hypothesis would be that the relationship does exist. If we get a p-value less than 0.05, we reject the null hypothesis that there is no relationship and conclude that there is indeed relationship between both variables.&lt;/p&gt;

&lt;h4 id=&#34;significance-interval&#34;&gt;Significance interval:&lt;/h4&gt;

&lt;h4 id=&#34;confidence-interval&#34;&gt;Confidence interval:&lt;/h4&gt;

&lt;p&gt;A confidence interval helps in assessing the margin of error of the prediction. Confidence intervals contain the point estimate of the population mean and the range represents the uncertainty about the where the population mean could lie.
As this point estimate is coming from one sample of data, confidence intervals represent that point estimates taken from different samples of data will lie between the lower and upper bounds of the confidence intervals.&lt;/p&gt;

&lt;p&gt;We also cannot calculate the probability of the population mean using the sample data.&lt;/p&gt;

&lt;p&gt;p-value and confidence intervals are used to assess the statistical significance of the results.&lt;br /&gt;
For our example, the P value (0.031) is less than the significance level (0.05), which indicates that our results are statistically significant. Similarly, our 95% confidence interval [267 394] does not include the null hypothesis mean of 260 and we draw the same conclusion.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-hypothesis-tests-confidence-intervals-and-confidence-levels&#34; target=&#34;_blank&#34;&gt;Reference&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;z-statistic&#34;&gt;Z-statistic:&lt;/h4&gt;

&lt;p&gt;Z-statistic is a standardized random sampling distribution of the x-values that are picked from a normal distribution. It reflects the number of standard deviations or standard errors that the value is away from the mean of the distribution.&lt;/p&gt;

&lt;p&gt;$$ Z = \frac{x - \mu}{\sigma}$$
where x is the value, $\mu$ is the mean and $\sigma$ is the standard deviation&lt;/p&gt;

&lt;h4 id=&#34;t-statistic&#34;&gt;t-statistic:&lt;/h4&gt;

&lt;p&gt;The T Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation.&lt;br /&gt;
When you run a hypothesis test, you use the T statistic with a p value. The p-value tells you what the odds are that your results could have happened by chance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is a T Statistic?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The T Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation.&lt;/p&gt;

&lt;p&gt;The T statistic doesn’t really tell you much on its own. It’s like the word “average” doesn’t mean anything on it’s own either, without some context. If I say “the average was 150,” it means nothing. If I say “the average weight of dogs seen in a veterinary office was 50lbs,” then the picture becomes clearer. In the same way, you need some more information along with your t statistic for it to make sense. You get this information by taking a sample and running a hypothesis test.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is the T Statistic used for?&lt;/strong&gt;&lt;br /&gt;
When you run a hypothesis test, you use the T statistic with a p value. The p-value tells you what the odds are that your results could have happened by chance. Let’s say you and a group of friends score an average of 205 on a bowling game. You know the average bowler scores 79.7. Should you and your friends consider professional bowling? Or are those scores a fluke? Finding the t statistic and the probability value will give you a good idea. More technically, finding those values will give you evidence of a significant difference between your team’s mean and the population mean (i.e. everyone).&lt;/p&gt;

&lt;p&gt;The greater the T, the more evidence you have that your team’s scores are significantly different from average. A smaller T value is evidence that your team’s score is not significantly different from average. It’s pretty obvious that your team’s score (205) is significantly different from 79.7, so you’d want to take a look at the probability value. If the p-value is larger than 5%, the odds are your team getting those scores are due to chance. Very small (under 5%), you’re onto something: think about going professional.&lt;/p&gt;

&lt;h4 id=&#34;t-score-vs-z-score&#34;&gt;T Score vs. Z-Score&lt;/h4&gt;

&lt;p&gt;The Z-score allows you to decide if your sample is different from the population mean. In order to use z, you must know four things:&lt;/p&gt;

&lt;p&gt;The population mean.
The population standard deviation.
The sample mean.
The sample size.
Usually in stats, you don’t know anything about a population, so instead of a Z score you use a T Test with a T Statistic. The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30).&lt;/p&gt;

&lt;h4 id=&#34;f-statistic&#34;&gt;F-statistic:&lt;/h4&gt;

&lt;p&gt;An F statistic is a value you get when you run an ANOVA test or a regression analysis to find out if the means between two populations are significantly different. It’s similar to a T statistic from a T-Test; A-T test will tell you if a single variable is statistically significant and an F test will tell you if a group of variables are jointly significant.&lt;br /&gt;
&lt;a href=&#34;https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/&#34; target=&#34;_blank&#34;&gt;Reference&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;F value = variance of the group means (Mean Square Between) / mean of the within group variances (Mean Squared Error)&lt;/p&gt;

&lt;h4 id=&#34;chi-square-statistic&#34;&gt;Chi-square statistic:&lt;/h4&gt;

&lt;h3 id=&#34;statistical-hypothesis-tests&#34;&gt;Statistical Hypothesis tests:&lt;/h3&gt;

&lt;p&gt;Along with python implementations or syntax:&lt;/p&gt;

&lt;h4 id=&#34;z-test&#34;&gt;Z-test:&lt;/h4&gt;

&lt;p&gt;For more information:&lt;/p&gt;

&lt;h4 id=&#34;t-test&#34;&gt;t-test:&lt;/h4&gt;

&lt;p&gt;For more information:
One sample test:&lt;br /&gt;
Independent sample test:
Pair sample test:&lt;/p&gt;

&lt;h4 id=&#34;anova&#34;&gt;ANOVA:&lt;/h4&gt;

&lt;p&gt;The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups.&lt;br /&gt;
&lt;a href=&#34;https://statistics.laerd.com/statistical-guides/one-way-anova-statistical-guide.php&#34; target=&#34;_blank&#34;&gt;Reference&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;How to run ANOVA in python?&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;chisquare-test&#34;&gt;Chisquare test:&lt;/h4&gt;

&lt;p&gt;For more information:&lt;/p&gt;

&lt;h4 id=&#34;pearson-correlation-test&#34;&gt;Pearson correlation test:&lt;/h4&gt;

&lt;p&gt;For more information:&lt;/p&gt;

&lt;h4 id=&#34;spearman-correlation-test&#34;&gt;Spearman correlation test:&lt;/h4&gt;

&lt;p&gt;For more information:&lt;/p&gt;

&lt;p&gt;References:&lt;br /&gt;
[1] &lt;a href=&#34;https://www.overleaf.com/learn/latex/Integrals,_sums_and_limits&#34; target=&#34;_blank&#34;&gt;https://www.overleaf.com/learn/latex/Integrals,_sums_and_limits&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&#34;https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/&#34; target=&#34;_blank&#34;&gt;https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/&lt;/a&gt;&lt;br /&gt;
[3] &lt;a href=&#34;https://docs.python.org/3/library/statistics.html&#34; target=&#34;_blank&#34;&gt;https://docs.python.org/3/library/statistics.html&lt;/a&gt;&lt;br /&gt;
[4] &lt;a href=&#34;https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f&#34; target=&#34;_blank&#34;&gt;https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f&lt;/a&gt;&lt;br /&gt;
[5] &lt;a href=&#34;https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/basic-statistics/inference/supporting-topics/basics/&#34; target=&#34;_blank&#34;&gt;https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/basic-statistics/inference/supporting-topics/basics/&lt;/a&gt;
[6]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ML Algorithms 1:Logistic Regression</title>
      <link>https://akhilesh-reddy.github.io/post/logistic-regression/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/post/logistic-regression/</guid>
      <description>

&lt;p&gt;This is my first post of a series of summaries of machine learning algorithms. I personally took this up to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others.
In this post, i would be talking about logistic regression which is still very widely used for classification in multiple industries.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://akhilesh-reddy.github.io/img/log reg.png&#34; alt=&#34;JCM&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Logistic regression is primarily a binary classifier which is used when the dependent variable is a categorical variable. Although, we know that linear regression is used for a dependent continuous variable,it might be tempting to use linear regression for a classification task. But we should be aware of the fact that final prediction of linear regression can go beyond 1 and be less than 0 which are not acceptable as a probability value.
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/logreg.jpg&#34; alt=&#34;JCM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So this takes us to the next step of logistic regression which is the logit function.&lt;/p&gt;

&lt;h3 id=&#34;mathematical-explanation&#34;&gt;Mathematical explanation&lt;/h3&gt;

&lt;p&gt;Following is a logit function in a matrix form with X being the independent variable matrix and Y being the probability value.
$${Pr}(Y=1/ {X}) = \frac{\text{e}^{\beta X}}{1 + \text{e}^{\beta X}}$$&lt;br/&gt;``&lt;/p&gt;

&lt;p&gt;It is also called sigmoid function.
&lt;img src=&#34;https://akhilesh-reddy.github.io/img/sigmoid.png&#34; alt=&#34;sigmoid&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Logit function restricts the final prediction value between 0 and 1 and would help in making the final classfication by using a threshold value. Lets go ahead and understand how logistic regression works.&lt;/p&gt;

&lt;p&gt;For simplicity, let us consider&lt;/p&gt;

&lt;p&gt;$${Pr}({X}) = {Pr}(Y = 1/{X})$$&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Now rearranging the logit equation we get,
$$\frac{{Pr}({X})}{1 - {Pr}({X})} = \text{e}^{\beta X}$$&lt;/p&gt;

&lt;p&gt;Now this will let us calculate the odds of a particular event happening and the value ranges from 0 to infinity. Taking log on both sides converts this into a linear equation which can be solved using maximum likelihood method.
$$\ln (\frac{{Pr}({X})}{1 - {Pr}({X})}) = \beta X$$&lt;/p&gt;

&lt;p&gt;In an situation with just one independent variable(x1), increasing the value of x1 changes the logodds by $\beta1$&lt;/p&gt;

&lt;p&gt;Types of logistic regression:
1. Binary logistic regreesion:  When there are only two putput classes
2. Multinomial : When there are more than 2 output classes&lt;br /&gt;
3. Ordinal : When the output classes are ordinal in nature like ratings(1 to 5)&lt;/p&gt;

&lt;h3 id=&#34;how-to-do-multiclass-logistic-regression-using-a-binary-logistic-regression&#34;&gt;How to do multiclass logistic regression using a binary logistic regression?&lt;/h3&gt;

&lt;p&gt;We will use all vs one approach to solve this problem.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say there are 4 classes in our dataset and 0,1,2,3 are their class labels&lt;br /&gt;
We will have to first train different logistic binary classifier in the following way.
1. We should fix one class as a reference class and will have to create 3 different binary classifiers one for each of the other classes.&lt;br /&gt;
2. Split the data into 3 parts, with each set containing the datapoints labelled with the selected 2 classes.&lt;br /&gt;
3. Build a classifier with classes 0 and 1 with 0 fixed as the reference class.&lt;br /&gt;
4. We will then obtain the parameter coefficient for the log odds of these 2 classes represented by the following equation.&lt;br /&gt;
&lt;center&gt; log(odds) = $\ \beta_1 X$&lt;/center&gt;&lt;br /&gt;
5. In a similar fashion, we build 2 other logistic classifiers and estimate the respective beta coefficients($\ \beta_2 $,$\ \beta_3 $) for those classes.&lt;/p&gt;

&lt;p&gt;Finally, it&amp;rsquo;s time to predict the class for a test data point.&lt;/p&gt;

&lt;p&gt;Every single data point is passed through each of these 3 classifiers and log odds are calculated through classifiers.&lt;/p&gt;

&lt;p&gt;There are 2 possible outcomes.&lt;br /&gt;
    1. If any of the logodds are +ve and also the highest among the other classifiers, then we choose that class as the final label.&lt;br /&gt;
    2. If all the log odds are -ve, then we choose the reference class, which is 0 in our case as the class label for the datapoint.&lt;/p&gt;

&lt;h3 id=&#34;keypoints-in-logistic-regression&#34;&gt;Keypoints in Logistic Regression:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Does NOT assume a linear relationship between the dependent variable and the independent variables, but it does assume linear relationship between the logit of the explanatory variables and the response.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Independent variables can be even the power terms or some other nonlinear transformations of the original independent variables&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The dependent variable does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,&amp;hellip;); binary logistic regression assume binomial distribution of the response&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The homogeneity of variance does NOT need to be satisfied&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Errors need to be independent but NOT normally distributed&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;It uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;implementation-in-r&#34;&gt;Implementation in R:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#### Creating train and test data

set.seed(99)
train &amp;lt;- createDataPartition(y=sel_features_dat_vf$y1,p=0.8,list=FALSE)
train_data &amp;lt;- sel_features_dat_vf[train,]
test_data &amp;lt;- sel_features_dat_vf[-train,]

set.seed(99)
model &amp;lt;- glm(train_data$y1 ~.,family=binomial(link=&#39;logit&#39;),data=train_data)
summary(model)

#### Assessing the predictive ability of the model

fitted.results &amp;lt;- predict(model,test_data[,1:17],type=&#39;response&#39;)
answers = test_data$y1


# Confusion matrix
cm_logistic &amp;lt;- table(answers, fitted.results&amp;gt;= 0.3)

head(fitted.results)

# ROC Curve

ROCRpred &amp;lt;- prediction(fitted.results, test_data$y1)
ROCRperf_log &amp;lt;- performance(ROCRpred, &#39;tpr&#39;,&#39;fpr&#39;)

p_roc_logistic &amp;lt;- plot(ROCRperf_log, text.adj = c(-0.2,1.7))

#Sensitivity and specificity
sens_spec_ROCR &amp;lt;- performance(ROCRpred, measure = &amp;quot;sens&amp;quot;,x.measure = &amp;quot;cutoff&amp;quot;)
plot(sens_spec_ROCR, text.adj = c(-0.2,1.7))

# Calculating the cutoff
cost.perf = performance(ROCRpred, &amp;quot;cost&amp;quot;)
ROCRpred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]

# AUC calculation
auc_ROCR &amp;lt;- performance(ROCRpred, measure = &amp;quot;auc&amp;quot;)
auc_ROCR

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;br /&gt;
[1] &lt;a href=&#34;https://newonlinecourses.science.psu.edu/stat504/node/164/&#34; target=&#34;_blank&#34;&gt;https://newonlinecourses.science.psu.edu/stat504/node/164/&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&#34;https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc&#34; target=&#34;_blank&#34;&gt;https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc&lt;/a&gt;&lt;br /&gt;
[3] &lt;a href=&#34;https://www.theanalysisfactor.com/the-distribution-of-dependent-variables/&#34; target=&#34;_blank&#34;&gt;https://www.theanalysisfactor.com/the-distribution-of-dependent-variables/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Association between grocery items using Apriori algorithm and Gephi visualization</title>
      <link>https://akhilesh-reddy.github.io/project/association-between-retail-items/</link>
      <pubDate>Fri, 19 Jan 2018 06:31:29 -0500</pubDate>
      
      <guid>https://akhilesh-reddy.github.io/project/association-between-retail-items/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
